# lecture
### stack
stack（堆栈）可以用list或array实现
list头部是堆栈出的地方，array尾部是堆栈出的地方
比较重要的想法是堆栈的底是可以随意变化的，如果我需要删除array栈的最后一位只需要把表示底的变量向前移即可
当计算时间复杂度时，我们需要计算的实际上是平均的时间复杂度，也就是说如果平均n次才需要复制一次，那么即使那次复制的时间复杂度是O(n)的，总的时间复杂度还是O(1)
但多复制实际还是用空间换时间，因为一次性开的越多占用的空间越大
### queue
链表实现，尾入头出，array实现一个类环形的，每次在尾部进入头部出时改变表示头地址的变量，尾部循环占用
### 时间、空间复杂度
一般来说重要的是worst-case或者average-case
比如二分法查找与线性查找，可以比较worst-case，average-case的计算需要根据输入来算
对于多项式形式的时间复杂度，只需要取最高次项即可
O代表了一个函数的上限，但是是多解的；θ代表了一个范围，包含上限和下限
这两个都是面对充分大情况的，接近0的地方不需要考虑
### 哈希表
dictionary的具体实现是通过哈希表实现的，dictionary中具有成对的两个变量（一般称为key与value）   这里比较准确的是dictionary和哈希表类似
哈希表实际上主要性质就是通过key在θ（1）的时间内找到value的数据结构
网络数据的读取是通过IP地址，不是域名
IP地址通过四个8bits的数字体现，数字属于0到255
后续对于哈希表会有很多其他要求，比如分布均匀等（减少重复赋值）具体见lec5的p21
python所有表类结构都具有哈希表的对应实现
哈希表实际上需要做的就是找到一个从变量到int的映射函数
左移右移按位与之类的操作有助于找到简单的哈希表key
二进制乘法可以较大概率避免相同key
大部分数学方法都是可以运用的，比如按位权重加法，取模，乘法或者多种一起用，但重复或者哈希表的崩溃总是不能避免的
每一次扩容需要重新建立哈希表（？），扩容默认扩两倍
哈希表关键在查找，不是存储，所以存储的规律性有助于查找的快速
负载因子（load factor）表示哈希表内部element数量和总空间的比值，一般为0.75左右
###### 解决哈希表冲突问题
chain链表解决，把所有有冲突的元素组成一个链表储存在同一个哈希表的位置
linear解决，有冲突时元素放在下一位，以此类推，末尾放完放开头（一种环链表的感觉）
quadratic解决，用一个公式来计算需要向前移动多少(确保所有位置都能检索到)
### 插入和冒泡排序
对于排序算法，时间复杂度不可能小于n
inplace表示不用额外空间
用二叉树来看，满的二叉树表示了排序的最低时间，所以理论上排序的时间复杂度不可能小于nlog(n)
插入排序在大部分地方都比冒泡好
###### 冒泡排序的时间复杂度
第一次比较n-1次，后续逐渐减少，实际上是$n^2$级别
###### 优化
学生提出的优化：用stack，从某一个标点上开始比较，大的进入stack，然后把进入的值作为比较值，重复上述过程。如果比较结束，stack中最上位就是最大值，然后取出，用stack中新的最上位作为比较值，从最大位的位置开始向后继续比较。(但我感觉不是优化，实际上时间复杂度是一样的)
###### 冒泡排序代码
```
int* bubble(int* a)
{
    int length = a.len();
    for (int i = 0; i < n-1; ++i)
    {
        for (int j = 0; j < n-1-i; ++j)
        {
            if (a[j] > a[j+1])
            {
                int temp;
                temp = a[j+1];
                a[j+1] = a[j];
                a[j] = temp;
            }
        }
    }
    return *a;
}
```
###### 插入排序代码
```
int* insert(int* a)
{
    int length = a.len();
    for (int i = 1; i < n; ++i)
    {
        for (int j = i; j > 0; --j)
        {
            if(a[j-1] > a[j])
            {
                std::swap(a[j-1], a[j])
            }
            else
            {
                break;
            }
        }
    }
}
```
### 归并排序
需要把一个数组分成若干个长度为1的数组，然后依次组成新数组
一般分割数组是二分
同时可以在数组小于6的条件下换用insert，会快一些
归并核心是递归，然后通过if判断数组长度来进行最后的回归
优势在于调整逆序对，可以一次调整多个，从而优于插入排序
归并排序的时间复杂度是nlogn
很多排序算法的出现都是把大的问题缩小，然后解决小问题
###### 排序稳定性
如果数组中有两个相等的元素，稳定性要求不交换这两个相等的元素的位置，原本在前的也在前，原本在后的还在后
### 快排
首先定位一个点的位置，其数据位n，然后把小于n的放在左边，大于n的放在右边，随后重复上述操作，再重新定位……
关键在于怎么找n，如果找到的n恰好是最小值或者最大值，快排就没有意义了，所以希望能找到中位数
确保选到中位数是不合适的，所以只要求尽量不接近头或尾；故而可以去头尾中三个数选中间的
后续填数字时是遍历数组，小于放开头大于放末尾
期望比较次数是数组中任意取两个数进行比较的概率的总和
任意i、j，ai、aj比较的概率是2/(j-i+1) j>i
快排需要n的空间，那么怎么inplace
可以让小的数字和大的数字互换，这样来做到inplace
实际空间复杂度在log(n)到n，因为每一次递归都需要一个额外的空间
平均时间复杂度为nlog(n)，最坏为$n^2$
### divide and conquer
分治法，把大的问题分成小的问题并解决
###### master therom
计算递归的时间复杂度
证明：
https://blog.csdn.net/jmh1996/article/details/82827579
###### 二进制整数乘法
最普通的二进制数乘法时间复杂度是$n^2$
试图用类似于merge归并的方式来减少时间复杂度，但最后没有改变
再优化小问题
原本对于二进制数A,B，先分为a,b,c,d，其中a,c为高位，b,d为低位
小问题就是计算ac,bd,ad,bc
而bc+ad = ac+bd-(a-b)(c-d)，可以用其他两个来计算
进而原本四个小问题被优化为三个小问题，时间复杂度优化成nlog(n)
###### 矩阵相乘
时间复杂度为$n^3$
同时根据线性代数，大矩阵相乘可以换成子矩阵的相乘
小问题就是子矩阵相乘，默认分成四份，把一个乘法拆成八个乘法（每两个需要相加）
再通过一些数学方式，强行做结合增加加法减少乘法，减少为7个乘法
关键在于怎么减少乘法
###### 卷积和快速傅里叶变换
对于优化快速傅里叶变换，可以看成两个多项式相乘的优化
### 树
一般用queue来实现bfs，用stack实现dfs
def查找可以通过recrusion做到，比如每次传入需要有parent节点和自己节点的信息，然后进行pre操作，比如print节点内容，随后若有child没有被找过则再调用dfs函数传入自己节点作为parent和child节点，最后进行post操作
```
def dfs (node, parent)
    pre-operations
    for child in node
        dfs (child, node)
    post-operations
```
forest是树的集合，遍历可以认为具有同一个root然后遍历，pre去首post去尾即可
###### 二叉树
full node代表这个节点有两个children
逆波兰表示法实际上可以看做stack，数字储存，操作符跳出两位运算再进入
完美二叉树（perfect）表示所有的节点都有两个children，高度相同
（complete）完全二叉树表示按照从左到右的顺序填满树，具体定义看L_11 p69
完全二叉树可以用数组存储，默认数组第0位不用（为了方便），具体构建见ppt
任意树转化为二叉树的方式：每一个node有两个指针，左指针指向第一个child，右指针指向下一个兄弟，从而把树转化成二叉树
### 堆
使用二叉树的结构来实现具有优先级的queue，构建树需要找到最小值作为root，然后随机从剩下的数中选取两个作为child。这样构成了一个基础的树，之后继续构建直接使用push即可
top就是root，时间复杂度为$\theta(1)$
pop需要移除root，然后比较child，选取小的成为新的root，再对child的child比较，重复直到选到leaf
push先作为leaf随机进入，然后和parent比较，如果小则两者交换位置，重复直到比parent大
pop和push的时间复杂度比较差的情况是$\theta(n)$的，因为有可能构建的树只有一条
所以需要balance，尽可能让树成为complete binary tree
优化的push：每一次从左侧开始，选取可能height最小的
优化的pop：把最右侧的leaf存到root的位置，然后下移重复原本的pop操作
比如使用array来实现时，我们知道数组的地址左移或者右移就可以得到left child或者parent，left child按位或1得到right child
array实现push可以直接加入到最后一位，pop直接把最后一位赋值给第一位
这样的最差时间复杂度也$\theta(ln(n))$，平均时间复杂度更小，只有$\theta(1)$
实际向上渗透(percolation)可以从上到下比较，渗透次数取决于可能向上的层数(h-k)乘以需要比较的数的个数，每向上一层需要比较2次
构建heap直接使用堆需要$\theta(nln(n))$(最差情况)，平均也需要$\theta(n)$
直接作为树来看，将其整理成heap，从底部开始向上修改，如果child比parent小则修改，每一次修改需要parent和较小的child更换
时间复杂度为$\theta(n)$ 
###### 弗洛伊德法建堆
先把所有元素任意的形式存入堆中，然后从最后一位开始查找，比较child和parent，如果逆序了那么交换，交换后查找该节点的child，无则跳过，**有则比较直到到达leaf**
###### 堆排序
构建堆，然后pop，$\theta(nln(n))$in worst case
使用最大堆(和上述情况相反)，修改pop为交换位置达到inplace
最小堆inplace排序得到降序排列，最大堆相反
### 霍夫曼编码
一种数据压缩储存的方式
相较于传统的每一个char存一个数据（等长），霍夫曼编码把相对出现较少的char编写更少的数据，出现较多的对应更多数据。
e.g，有ABCD四个字母，AD少出现，BC多出现，那么把B编成10，C为0，A为110，D为111。
霍夫曼编码的目标是缩小空间
霍夫曼编码通过堆来实现，因为堆排序的不确定性(每次排序可以向左也可以向右)来保证编码的保密性
怎么实现字符和数据的对应?首先构建一个升序堆，这个堆内部所有元素是出现过的字符；再构建一个二叉树，使得所有属于升序堆的元素都是这个树的leaf，出现频率越高元素成为leaf的height越低
具体见lec13.1 P37-38
霍夫曼编码能够确保解码的唯一性，因为所有的字符元素都是leaf，比如一段数据是1011，假设字符e是10，那么不会出现编码是101或1011的元素，因为101和1011是e的子孙，而e没有子孙。
霍夫曼编码树必须是full binary tree
### 二叉树搜索
BST要求所有左子树小于root，所有右子树大于root，对于所有子树都成立，顺序输出就是in-order排序
查找实际上就类似于二项查找
每一次插入会有一个相对固定的位置，根据节点的祖先来确定范围
删除元素分三种情况，删除leaf，删除有一个child的node，或者删除有两个child的node。删除leaf直接删除，删除一个child让child取代被删除的点的位置，这两个都相对简单。
删除两个child需要找到一个node最接近root。寻找左子树最大值或者右子树最小值都可以实现。
### AVL树
AVL树实现的是让BST更加balanced
要求左右子树高度差不能超过1，对于所有的subtree也一样
AVL树最多有$2^{h+1}-1$个node，最少用递归来算(所有子树都是AVL树)
F(h)=F(h-1)+F(h-2)+1
这会变成一个斐波那契数列，之后有F(h) $\approx 1.8944\phi^h - 1$,$\phi$约等于黄金分割
当插入元素之后导致左右子树高度相差超过1后，先向上找到最先有问题的点，然后进行交换，在交换时可以把一个确定的AVL子树看做一个node去交换，具体见L14 P46
把root下一个需要改变的子树root变成树的root，原本子树的右子树变为原本root(现在右子树的root)的右子树
以上操作要求是新节点在左子树，但如果在右子树时，以上操作就不满足了
具体操作见L14 P64
实际上是root向下的三个节点(形成3-1-2这样树的三个node)交换位置，然后分配原本最下方的左右子树
(以上情况都是左子树的右子树或者左子树，然后右子树的左子树或者右子树也是一样的)
注意需要确定是哪一个节点出现问题
实际上需要存储一个height
### 红黑树
代码中把所有点赋为0或1来代表颜色
null path：从root开始到任意节点要求path上所有的点没有两个children
要求：root为黑色，若一个点是红色，它的child一定是黑色，各个null path上黑色点的数量相等
规律:
    1、所有红点不可能恰好有一个child(证明见L15.1 P8)
    2、若P点只有一个child，P一定是黑的，child一定是红色并且是leaf(null path的要求)
对于node数量最少的红黑树，可以不断在root上加红leaf黑root并且补充黑node构成的perfect子树
###### 插入
如果在尾部作为leaf插入，那么插入节点一定是红色的
插入方法见L15.1 P39开始，核心就是每次插入时如果插入点的grandparent只有一个红色的child，插入后调整让parent有两个红child(实际上是在AVLtree的基础上建立的，所以需要经历变色过程因为每个节点有value)
如果插入点的grandparent有两个红child，那么会把grandparent变红，它的children变黑，并向上递归检查是否有需要经过同样过程变色的点(即它的parent颜色是否是红色的)，直到达到root
最后所有调整都是AVL tree的调整
红黑树还有从top向下的调整
原本插入操作不会改变，只有遇到有两个红色孩子时交换Parent和child颜色的互换
每一次颜色互换之后如果遇到parent的parent也是红色，那么需要旋转
每一次插入都需要从插入节点向上查找有没有需要调整的点，所以先查找再插入
向上查找会一直查找到root
### 不相交集合(disjoint sets)
find 如果两个元素处在同一集合中，则find(a)==find(b)
一种类似于哈希表的方式，先遍历总集并赋予每个元素一个值来代表属于的集合
这样find的时间复杂度就是O(1)
但是合并两个子集很麻烦，所以用树的形式来存储，合并两个子集时让一个子集的代表值加入另一个集合中,这样时间复杂度会从O(1)变为O(h)



### 图(graph)
两个点构成一个集合代表两点之间有边，比如{A,B}和{B,C}
degree代表一个点与多少个点相邻(属于多少集合)
存储图的方式：矩阵存储，简单但空间复杂度高
或者使用链表存储，每个点有一个链表存储所有与它相连的点
path是一个有序点集
### 图的搜索
###### 广度优先
实现方式和树差不多，访问每一个vertex，标记vertex为已访问并且将他push_back到一个queue中
如果在遍历过程中queue是非空的，那么queue需要pop首位vertex，并将这个vertex的未访问的相邻点全部标记为已访问并push_back进queue
###### 深度优先
同理，访问每一个vertex，标记为已访问并push
如果stack非空，pop首位并且push相邻且未访问的一个vertex(随机)
###### 遍历不连接图
可以使用map的形式，每次随机选取一个节点进行遍历，并将遍历到的点的value更改为首个遍历的节点，若全部遍历完成并且还有vertex没有访问到再随机选取，具体可见L17_P64

### 最小生成树
###### Prim’s Algorithm
随机设立一个起始点，然后选取距离该图最近(edge权重最小的)的vertex，两者结合成为一个新的图；再对于这个图选取edge权重最小的vertex，重复操作直到所有点都被包含在图中
时间复杂度有两种：   A binary heap:	O(|E| ln(|V|))
                    A Fibonacci heap: O(|E| + |V|ln(|V|))
斐波那契堆的性质是：Pop is still O(ln(|V|)), but inserting and moving a key is Q(1)
时间复杂度分析实际上是找到最小边的时间复杂度
###### Kruskal’s Algorithm
将边排序，然后从小到大依次加入最小生成树，如果构成循环那么跳过这个边
时间复杂度是 O(|E| ln(|E|))
主要就是排序的时间以及循环检查，也就是用disjoint set中的find来确认两个点是否已经在一个子树下

这两种算法对于负权值图同样成立

### 贪心算法
每一次只专注于一部分的解，并且在这部分中每次选取最优的部分
贪心算法不是一直有效
关键在于怎么定义贪心，什么才会是最优解
贪心算法的证明也是关键，提供了三种方式

### 最短路径问题
###### Dijkstra’s algorithm
时间复杂度：
用最小堆： O(ElnV)
用斐波那契堆： O(E+VlnV)
###### Bellman-Ford algorithm
遍历到V点时，需要遍历所有V的邻居U求min{dist(v),dist(u)+或-e(u,v)}
加还是减取决于是U到V是in还是out
对于图的要求很低
时间复杂度为O(V·E)
###### A*
类似于Dijkstra，A*将距离分为多个部分，一部分是标准值(自己设定的标准，一般代表下界)，另一部分是已知距离
每一次选取A所有的邻居点中w(v)最短的，w(v)=d(a,neighbour)+h(v,neighbour)
d代表已知距离，h代表标准值
对于树或图有两种搜索方式，但实际上只是多加一个是否访问的变量
相比于Dijkstra，会更有效率地接近目标点